# ğŸ¬ Netflix Data Analysis End-to-End Project (AWS S3 â†’ Snowflake â†’ dbt)

![AWS to Snowflake](images/AWS_to_Snowflake.PNG)

![Status](https://img.shields.io/badge/Status-Active-success)
![DBT](https://img.shields.io/badge/Tool-dbt-FF694B?logo=dbt&logoColor=white)
![Snowflake](https://img.shields.io/badge/Data%20Warehouse-Snowflake-29B5E8?logo=snowflake&logoColor=white)
![AWS](https://img.shields.io/badge/Storage-AWS%20S3-FF9900?logo=amazon-aws&logoColor=white)
![Python](https://img.shields.io/badge/Language-Python-blue?logo=python)
![SQL](https://img.shields.io/badge/Language-SQL-lightgrey?logo=sqlite)

---

## ğŸ“š Table of Contents
- [ğŸ“– Overview](#-project-overview)
- [ğŸ§± Architecture](#-architecture-diagram)
- [ğŸª£ Step 1: Data Ingestion](#-step-1-data-ingestion--aws-s3--snowflake)
- [ğŸ§© Step 2: Initialize dbt Project](#-step-2-initialize-dbt-project)
- [ğŸ§™â€â™‚ï¸ Step 3: Staging and Transformations](#-step-3-staging-and-transformations)
- [ğŸ§® Step 4: Building Fact & Dimension Models](#-step-4-building-fact--dimension-models)
- [âš™ï¸ Step 5: Incremental Loads & Ephemeral Models](#ï¸-step-5-incremental-loads--ephemeral-models)
- [ğŸ•’ Step 6: Slowly Changing Dimensions (SCD)](#-step-6-slowly-changing-dimensions-scd)
- [ğŸ§  Step 7: Macros, Seeds & Sources](#-step-7-macros-seeds--sources)
- [âœ… Step 8: Testing & Documentation](#-step-8-testing--documentation)
- [ğŸ“Š Step 9: Analysis & Gold Layer](#-step-9-analysis--gold-layer)
- [ğŸš€ How to Run This Project](#-how-to-run-this-project)
- [ğŸ§­ Project Highlights](#-project-highlights)
- [ğŸ§° Tools & Technologies](#-tools--technologies)
- [ğŸ§‘â€ğŸ’» Author & Contact](#-author--contact)
- [ğŸ Conclusion](#-conclusion)

---

## ğŸ“– Project Overview

This project demonstrates a **complete data engineering and analytics pipeline** using **AWS S3**, **Snowflake**, and **dbt (Data Build Tool)** â€” from **raw CSV ingestion** to **gold-level analytics and visualization-ready models**.

It replicates a **real-world Netflix data workflow**, showcasing how to move, transform, test, and document data efficiently using modern data stack principles.

**Medium Article:**  
[Building an End-to-End Data Pipeline with dbt, Snowflake & AWS â€” The Netflix Data Analysis Project](https://medium.com/@codegnerdev/building-an-end-to-end-data-pipeline-with-dbt-snowflake-aws-the-netflix-data-analysis-project-bc26c1825e52)

---

## ğŸ§± Architecture Diagram

![Snowflake User Creation](images/snowflake_user_creation.PNG)
![src_table](images/src_table.PNG)
![raw_movies_table](images/raw_movies_table.PNG)
![insert](images/insert.PNG)
![views_on_snowflake](images/views_on_snowflake.PNG)

---

## ğŸª£ Step 1: Data Ingestion â€” AWS S3 â†’ Snowflake

The raw Netflix data (CSV files) is stored in **AWS S3**.  
A **Snowflake stage** is created to connect and load this data using the `COPY INTO` command.

```sql
COPY INTO raw.netflix_data
FROM @aws_stage/netflix_data/
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY='"' SKIP_HEADER = 1);
````

### âœ… Steps:

* Create **Snowflake user** and **warehouse**
* Define **stages** to pull data from AWS
* Load data using **Snowflake COPY command**

![AWS to Snowflake](images/AWS_to_Snowflake.PNG)

---

## ğŸ§© Step 2: Initialize dbt Project

![dbt Init Netflix](images/dbt_init_netflix.PNG)

We initialize a new dbt project for Netflix using:

```bash
dbt init netflix_project
```

Then configure connections to **Snowflake** via the `profiles.yml`.

### Key Folders:

* `models/` â†’ Transformations
* `seeds/` â†’ Static data
* `snapshots/` â†’ Change tracking
* `macros/` â†’ Reusable SQL logic

---

## ğŸ§™â€â™‚ï¸ Step 3: Staging and Transformations

All raw data is **staged** into clean, standardized tables using dbt models.

```bash
dbt run
```

This creates **views** in Snowflake (logical transformations).
Views donâ€™t store data â€” they simply reference SQL queries dynamically.

![Views on dbt](images/views_on_dbt.PNG)
![views](images/views.PNG)

---

## ğŸ§® Step 4: Building Fact & Dimension Models

We move from staging to **data modeling**.
Instead of using the raw schema, we now use the **DEV schema**.

### Models:

* **Dimension Tables:** e.g., `dim_users`, `dim_movies`
* **Fact Table:** `fct_ratings` (stores incremental logic)

![Lineage Graph](images/Lineage_Graph.PNG)

---

## âš™ï¸ Step 5: Incremental Loads & Ephemeral Models

For large datasets, we use **incremental materialization** in dbt.
This ensures only new or updated records are processed.

```sql
{{ config(materialized='incremental', unique_key='rating_id') }}
```

Ephemeral models are used for temporary computations that run in memory.

![fct\_ratings](images/test.PNG)

---

## ğŸ•’ Step 6: Slowly Changing Dimensions (SCD)

We implement **SCD1**, **SCD2**, and **SCD3/6** to handle data changes over time.

Example surrogate key:

```sql
{{ dbt_utils.generate_surrogate_key(['user_id','movie_id','tag']) }} AS row_key
```

![snapshots](images/snapshots.PNG)
![snaptag](images/snaptag.PNG)

---

## ğŸ§  Step 7: Macros, Seeds & Sources

### ğŸª„ Macros

Reusable SQL snippets that simplify testing and transformation logic.

![macros](images/macros.PNG)

### ğŸŒ± Seeds

Static reference datasets are loaded using:

```bash
dbt seed
```

### ğŸ“„ Sources

Defined in `sources.yml` to track data lineage.

---

## âœ… Step 8: Testing & Documentation

We use both **schema** and **singular** tests to validate data integrity.

```bash
dbt test
```

![test](images/test.PNG)

For documentation:

```bash
dbt docs generate
dbt docs serve
```

![dbt Serve](images/dbt_serve.PNG)
![dbt Docs](images/dybt_docs.PNG)

---

## ğŸ“Š Step 9: Analysis & Gold Layer

The **Gold Layer** delivers business-ready analytics and insights.

![analysis](images/analysis.PNG)

### Analyses Include:

* ğŸï¸ Movies with â‰¥100 Ratings
* ğŸ­ Genre Rating Distribution
* ğŸ‘¥ User Engagement
* ğŸ“ˆ Movie Release Trends
* ğŸ·ï¸ Tag Relevance
* ğŸ† Top 10 Movies by Genre
* ğŸ“… Monthly Rating Trends

**Files in `models/gold/`:**

```
movie_analysis.sql
genre_ratings.sql
user_engagement.sql
release_trends.sql
tag_relevance.sql
top10_by_genre.sql
monthly_trends.sql
```

![title\_vs\_av\_rating](images/title_vs_av_rating.PNG)
![title\_vs\_tot\_rating](images/title_vs_tot_rating.PNG)
![relevant\_score\_test](images/relevant_score_test.PNG)

---

## ğŸš€ How to Run This Project

Follow these steps to replicate this project locally or in your own environment.

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/KibutuJr/Netflix-dbt-project.git
cd Netflix-dbt-project
```

### 2ï¸âƒ£ Set Up Virtual Environment (Optional but Recommended)

```bash
python -m venv venv
source venv/bin/activate      # (Mac/Linux)
venv\Scripts\activate         # (Windows)
```

### 3ï¸âƒ£ Install dbt and Dependencies

```bash
pip install dbt-snowflake dbt-core
```

### 4ï¸âƒ£ Configure dbt Profile

Update your `profiles.yml` file with your **Snowflake credentials**:

```yaml
netflix_project:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: <your_account>
      user: <your_username>
      password: <your_password>
      role: <your_role>
      database: MOVIELENS
      warehouse: COMPUTE_WH
      schema: DEV
```

### 5ï¸âƒ£ Run dbt Commands

```bash
dbt debug             # Test connection
dbt seed              # Load seed files
dbt run               # Execute models
dbt test              # Validate data
dbt docs generate     # Generate documentation
dbt docs serve        # Open docs locally
```

### 6ï¸âƒ£ Verify in Snowflake

Check the **DEV schema** to view your staged, transformed, and gold-layer tables.

---

## ğŸ§­ Project Highlights

* **End-to-End Data Flow:** AWS S3 â†’ Snowflake â†’ dbt
* **Modern Data Stack:** Modular, scalable, cloud-native
* **Incremental Models:** Optimized performance
* **Snapshots & SCDs:** Full data versioning
* **Automated Testing & Docs:** dbt-native QA
* **Ad-hoc Analysis:** Gold-layer insights

---

## ğŸ§° Tools & Technologies

| Tool          | Purpose                        |
| ------------- | ------------------------------ |
| **AWS S3**    | Raw data storage               |
| **Snowflake** | Cloud data warehouse           |
| **dbt**       | Data modeling & transformation |
| **SQL**       | Querying & transformations     |
| **Python**    | Data prep and automation       |

---

## ğŸ§‘â€ğŸ’» Author & Contact

ğŸ‘¤ **Fred Kibutu**
ğŸŒ [Portfolio](https://kibutujr.vercel.app/)
ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/fred-kibutu/)
ğŸ“§ [Email](mailto:kibutujr@gmail.com)

---

## ğŸ Conclusion

This project showcases a **real-world dbt-driven Netflix data pipeline**, transforming raw datasets into actionable insights.
It reflects how modern data teams use **dbt, Snowflake, and AWS** to power analytics at scale â€” from **data ingestion** to **business-ready metrics**.

---

âœ¨ *"Data isnâ€™t just numbers â€” itâ€™s the story behind every play, pause, and rating."* ğŸ¥
